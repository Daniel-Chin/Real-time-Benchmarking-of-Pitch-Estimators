{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified from Crepe  \n",
    "\n",
    "CREPE: A Convolutional Representation for Pitch Estimation  \n",
    "Jong Wook Kim, Justin Salamon, Peter Li, Juan Pablo Bello.  \n",
    "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from resampy import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CAPACITY = 'full'\n",
    "\n",
    "model_srate = 16000\n",
    "\n",
    "CENTS_MAPPING = np.linspace(0, 7180, 360) + 1997.3794084376191"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_a, sr = librosa.load('a.wav')\n",
    "raw_i, sr = librosa.load('i.wav')\n",
    "print('sr\\'', sr)\n",
    "raw_a = resample(raw_a, sr, model_srate)[:1024]\n",
    "raw_i = resample(raw_i, sr, model_srate)[:1024]\n",
    "sr = model_srate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_load_model():\n",
    "    from tensorflow.keras.layers import Input, Reshape, Conv2D, BatchNormalization\n",
    "    from tensorflow.keras.layers import MaxPool2D, Dropout, Permute, Flatten, Dense\n",
    "    from tensorflow.keras.models import Model\n",
    "\n",
    "    capacity_multiplier = {\n",
    "        'tiny': 4, 'small': 8, 'medium': 16, 'large': 24, 'full': 32\n",
    "    }[MODEL_CAPACITY]\n",
    "\n",
    "    layers = [1, 2, 3, 4, 5, 6]\n",
    "    filters = [n * capacity_multiplier for n in [32, 4, 4, 4, 8, 16]]\n",
    "    widths = [512, 64, 64, 64, 64, 64]\n",
    "    strides = [(4, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
    "\n",
    "    x = Input(shape=(1024,), name='input', dtype='float32')\n",
    "    y = Reshape(target_shape=(1024, 1, 1), name='input-reshape')(x)\n",
    "\n",
    "    for l, f, w, s in zip(layers, filters, widths, strides):\n",
    "        y = Conv2D(f, (w, 1), strides=s, padding='same',\n",
    "                   activation='relu', name=\"conv%d\" % l)(y)\n",
    "        y = BatchNormalization(name=\"conv%d-BN\" % l)(y)\n",
    "        y = MaxPool2D(pool_size=(2, 1), strides=None, padding='valid',\n",
    "                      name=\"conv%d-maxpool\" % l)(y)\n",
    "        y = Dropout(0.25, name=\"conv%d-dropout\" % l)(y)\n",
    "\n",
    "    y = Permute((2, 1, 3), name=\"transpose\")(y)\n",
    "    y = Flatten(name=\"flatten\")(y)\n",
    "    y = Dense(360, activation='sigmoid', name=\"classifier\")(y)\n",
    "\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "\n",
    "#     package_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "    package_dir = 'D:\\\\Programs\\\\Anaconda\\\\Lib\\\\site-packages\\\\crepe'\n",
    "    filename = \"model-{}.h5\".format(MODEL_CAPACITY)\n",
    "    model.load_weights(os.path.join(package_dir, filename))\n",
    "    model.compile('adam', 'binary_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_and_load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(y, sr):\n",
    "    if sr != model_srate:\n",
    "        print('Sample rate incorrect')\n",
    "        raise Exception()\n",
    "    assert len(y) == 1024\n",
    "\n",
    "    # normalize each frame -- this is expected by the model\n",
    "    y -= np.mean(y)\n",
    "    y /= np.std(y)\n",
    "\n",
    "    # run prediction and convert the frequency bin weights to Hz\n",
    "    return model.predict(y[None, :])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_local_average_cents(salience):\n",
    "    center = int(np.argmax(salience))\n",
    "    start = max(0, center - 4)\n",
    "    end = min(len(salience), center + 5)\n",
    "    salience = salience[start:end]\n",
    "    product_sum = np.sum(\n",
    "        salience * CENTS_MAPPING[start:end])\n",
    "    weight_sum = np.sum(salience)\n",
    "    return product_sum / weight_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(audio, sr):\n",
    "    activation = get_activation(audio, sr)\n",
    "    confidence = activation.max()\n",
    "\n",
    "    cents = to_local_average_cents(activation)\n",
    "\n",
    "    frequency = 10 * 2 ** (cents / 1200)\n",
    "\n",
    "    return frequency, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(raw_a, sr))\n",
    "print(predict(raw_i, sr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
